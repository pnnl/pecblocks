(base) WE30866:hwpv mcde601$ ./train_bal.sh
model_folder = ./big3
model_root = big3
data_path = /Users/mcde601/Documents/data/big3.hdf5
idx_in [0, 1, 2, 3, 4, 5, 6, 7, 8]
idx_out [9, 10, 11, 12]
read 23400 dataframes
dt=0.002000 data_len=3500 n_io=13 n_case=23400
['T', 'G', 'Fc', 'Md', 'Mq', 'Vd', 'Vq', 'GVrms', 'Ctl'] ['Vdc', 'Idc', 'Id', 'Iq'] (23400, 3500, 13)
shapes of t (3500,) data_train (23400, 3500, 13) n_in=9, n_out=4
t range 0.000000 to 6.998000
Before Scaling:
Column       Min       Max      Mean     Range
T         15.000    35.003    25.000    20.003
G         -0.000   999.995   431.680   999.995
Fc        55.000    65.000    60.000    10.000
Md         0.750     1.120     0.969     0.370
Mq        -0.550     0.550     0.000     1.100
Vd        -0.000   469.715   291.007   469.715
Vq      -211.352   181.357    -3.793   392.709
GVrms     -0.000   575.341   193.854   575.341
Ctl        0.000     1.000     0.714     1.000
Vdc       -0.000   431.964   264.630   431.964
Idc       -0.000   294.805   122.579   294.805
Id        -0.000   201.934    72.782   201.934
Iq       -87.411    79.125    -1.626   166.536
After Scaling:
Column       Min       Max      Mean     Range     Scale    Offset
T         -0.500     0.500     0.000     1.000    20.003    25.000
G         -0.432     0.568    -0.000     1.000   999.995   431.680
Fc        -0.500     0.500     0.000     1.000    10.000    60.000
Md        -0.593     0.407    -0.000     1.000     0.370     0.969
Mq        -0.500     0.500     0.000     1.000     1.100     0.000
Vd        -0.620     0.380     0.000     1.000   469.715   291.007
Vq        -0.529     0.471    -0.000     1.000   392.709    -3.793
GVrms     -0.337     0.663    -0.000     1.000   575.341   193.854
Ctl       -0.714     0.286     0.000     1.000     1.000     0.714
Vdc       -0.613     0.387    -0.000     1.000   431.964   264.630
Idc       -0.416     0.584     0.000     1.000   294.805   122.579
Id        -0.360     0.640     0.000     1.000   201.934    72.782
Iq        -0.515     0.485    -0.000     1.000   166.536    -1.626
make_mimo_block iir
Dataset split: 23400 21060 2340 validation_scale=9.000
Epoch    0 of  240 | Training Loss     5.164559 | Validation Loss     0.591322
Epoch    2 of  240 | Training Loss     0.297005 | Validation Loss     0.221932
Epoch    4 of  240 | Training Loss     0.177814 | Validation Loss     0.153846
Epoch    6 of  240 | Training Loss     0.151936 | Validation Loss     0.158312
Epoch    8 of  240 | Training Loss     0.134889 | Validation Loss     0.124797
Epoch   10 of  240 | Training Loss     0.124776 | Validation Loss     0.120251
Epoch   12 of  240 | Training Loss     0.120165 | Validation Loss     0.106838
Epoch   14 of  240 | Training Loss     0.113800 | Validation Loss     0.112269
Epoch   16 of  240 | Training Loss     0.109566 | Validation Loss     0.101215
Epoch   18 of  240 | Training Loss     0.105207 | Validation Loss     0.102460
Epoch   20 of  240 | Training Loss     0.103342 | Validation Loss     0.093569
Epoch   22 of  240 | Training Loss     0.101241 | Validation Loss     0.089868
Epoch   24 of  240 | Training Loss     0.097352 | Validation Loss     0.095517
Epoch   26 of  240 | Training Loss     0.094921 | Validation Loss     0.103053
Epoch   28 of  240 | Training Loss     0.092372 | Validation Loss     0.087820
Epoch   30 of  240 | Training Loss     0.089621 | Validation Loss     0.085149
Epoch   32 of  240 | Training Loss     0.086563 | Validation Loss     0.079375
Epoch   34 of  240 | Training Loss     0.084424 | Validation Loss     0.079952
Epoch   36 of  240 | Training Loss     0.082539 | Validation Loss     0.081128
Epoch   38 of  240 | Training Loss     0.081301 | Validation Loss     0.074506
Epoch   40 of  240 | Training Loss     0.080316 | Validation Loss     0.070093
Epoch   42 of  240 | Training Loss     0.080156 | Validation Loss     0.077426
Epoch   44 of  240 | Training Loss     0.078632 | Validation Loss     0.068272
Epoch   46 of  240 | Training Loss     0.078053 | Validation Loss     0.076009
Epoch   48 of  240 | Training Loss     0.078110 | Validation Loss     0.069936
Epoch   50 of  240 | Training Loss     0.076188 | Validation Loss     0.073655
Epoch   52 of  240 | Training Loss     0.075032 | Validation Loss     0.083839
Epoch   54 of  240 | Training Loss     0.075482 | Validation Loss     0.073571
Epoch   56 of  240 | Training Loss     0.074375 | Validation Loss     0.074980
Epoch   58 of  240 | Training Loss     0.074205 | Validation Loss     0.077903
Epoch   60 of  240 | Training Loss     0.074015 | Validation Loss     0.078144
Epoch   62 of  240 | Training Loss     0.072859 | Validation Loss     0.084076
Epoch   64 of  240 | Training Loss     0.072676 | Validation Loss     0.068665
Epoch   66 of  240 | Training Loss     0.072930 | Validation Loss     0.071422
Epoch   68 of  240 | Training Loss     0.071918 | Validation Loss     0.072442
Epoch   70 of  240 | Training Loss     0.072074 | Validation Loss     0.070970
Epoch   72 of  240 | Training Loss     0.071434 | Validation Loss     0.064622
Epoch   74 of  240 | Training Loss     0.071432 | Validation Loss     0.070011
Epoch   76 of  240 | Training Loss     0.069714 | Validation Loss     0.083417
Epoch   78 of  240 | Training Loss     0.069894 | Validation Loss     0.079080
Epoch   80 of  240 | Training Loss     0.070566 | Validation Loss     0.078152
Epoch   82 of  240 | Training Loss     0.070374 | Validation Loss     0.067003
Epoch   84 of  240 | Training Loss     0.070584 | Validation Loss     0.065638
Epoch   86 of  240 | Training Loss     0.069514 | Validation Loss     0.072291
Epoch   88 of  240 | Training Loss     0.069804 | Validation Loss     0.073849
Epoch   90 of  240 | Training Loss     0.068370 | Validation Loss     0.065477
Epoch   92 of  240 | Training Loss     0.069074 | Validation Loss     0.066278
Epoch   94 of  240 | Training Loss     0.068692 | Validation Loss     0.072026
Epoch   96 of  240 | Training Loss     0.068507 | Validation Loss     0.074361
Epoch   98 of  240 | Training Loss     0.068720 | Validation Loss     0.070812
Epoch  100 of  240 | Training Loss     0.068449 | Validation Loss     0.063119
Epoch  102 of  240 | Training Loss     0.068366 | Validation Loss     0.062252
Epoch  104 of  240 | Training Loss     0.068108 | Validation Loss     0.062925
Epoch  106 of  240 | Training Loss     0.067788 | Validation Loss     0.067143
Epoch  108 of  240 | Training Loss     0.068102 | Validation Loss     0.070435
Epoch  110 of  240 | Training Loss     0.067397 | Validation Loss     0.060093
Epoch  112 of  240 | Training Loss     0.066781 | Validation Loss     0.060901
Epoch  114 of  240 | Training Loss     0.066809 | Validation Loss     0.061381
Epoch  116 of  240 | Training Loss     0.066739 | Validation Loss     0.073350
Epoch  118 of  240 | Training Loss     0.066643 | Validation Loss     0.063199
Epoch  120 of  240 | Training Loss     0.066676 | Validation Loss     0.064757
Epoch  122 of  240 | Training Loss     0.066816 | Validation Loss     0.063425
Epoch  124 of  240 | Training Loss     0.067477 | Validation Loss     0.063114
Epoch  126 of  240 | Training Loss     0.065764 | Validation Loss     0.060718
Epoch  128 of  240 | Training Loss     0.065888 | Validation Loss     0.062575
Epoch  130 of  240 | Training Loss     0.066407 | Validation Loss     0.062952
Epoch  132 of  240 | Training Loss     0.065389 | Validation Loss     0.065802
Epoch  134 of  240 | Training Loss     0.065708 | Validation Loss     0.064974
Epoch  136 of  240 | Training Loss     0.066075 | Validation Loss     0.063812
Epoch  138 of  240 | Training Loss     0.065576 | Validation Loss     0.060454
Epoch  140 of  240 | Training Loss     0.065830 | Validation Loss     0.064250
Epoch  142 of  240 | Training Loss     0.065829 | Validation Loss     0.066501
Epoch  144 of  240 | Training Loss     0.065949 | Validation Loss     0.060066
Epoch  146 of  240 | Training Loss     0.065705 | Validation Loss     0.064461
Epoch  148 of  240 | Training Loss     0.065652 | Validation Loss     0.065615
Epoch  150 of  240 | Training Loss     0.064938 | Validation Loss     0.063246
Epoch  152 of  240 | Training Loss     0.065364 | Validation Loss     0.073785
Epoch  154 of  240 | Training Loss     0.064807 | Validation Loss     0.061933
Epoch  156 of  240 | Training Loss     0.065319 | Validation Loss     0.060344
Epoch  158 of  240 | Training Loss     0.064604 | Validation Loss     0.063424
Epoch  160 of  240 | Training Loss     0.064163 | Validation Loss     0.064223
Epoch  162 of  240 | Training Loss     0.064082 | Validation Loss     0.058106
Epoch  164 of  240 | Training Loss     0.064818 | Validation Loss     0.061801
Epoch  166 of  240 | Training Loss     0.064688 | Validation Loss     0.061504
Epoch  168 of  240 | Training Loss     0.064221 | Validation Loss     0.064842
Epoch  170 of  240 | Training Loss     0.063944 | Validation Loss     0.062928
Epoch  172 of  240 | Training Loss     0.063957 | Validation Loss     0.069563
Epoch  174 of  240 | Training Loss     0.063733 | Validation Loss     0.063551
Epoch  176 of  240 | Training Loss     0.063962 | Validation Loss     0.063764
Epoch  178 of  240 | Training Loss     0.063587 | Validation Loss     0.061001
Epoch  180 of  240 | Training Loss     0.063421 | Validation Loss     0.061833
Epoch  182 of  240 | Training Loss     0.063265 | Validation Loss     0.057366
Epoch  184 of  240 | Training Loss     0.063270 | Validation Loss     0.060487
Epoch  186 of  240 | Training Loss     0.063159 | Validation Loss     0.061005
Epoch  188 of  240 | Training Loss     0.063245 | Validation Loss     0.059912
Epoch  190 of  240 | Training Loss     0.062860 | Validation Loss     0.066776
Epoch  192 of  240 | Training Loss     0.063253 | Validation Loss     0.063092
Epoch  194 of  240 | Training Loss     0.062161 | Validation Loss     0.054978
Epoch  196 of  240 | Training Loss     0.063059 | Validation Loss     0.062090
Epoch  198 of  240 | Training Loss     0.062430 | Validation Loss     0.059396
Epoch  200 of  240 | Training Loss     0.062497 | Validation Loss     0.060441
Epoch  202 of  240 | Training Loss     0.061687 | Validation Loss     0.058300
Epoch  204 of  240 | Training Loss     0.062531 | Validation Loss     0.057096
Epoch  206 of  240 | Training Loss     0.062426 | Validation Loss     0.060077
Epoch  208 of  240 | Training Loss     0.062449 | Validation Loss     0.059481
Epoch  210 of  240 | Training Loss     0.062002 | Validation Loss     0.063061
Epoch  212 of  240 | Training Loss     0.061955 | Validation Loss     0.069781
Epoch  214 of  240 | Training Loss     0.062094 | Validation Loss     0.060417
Epoch  216 of  240 | Training Loss     0.062261 | Validation Loss     0.060315
Epoch  218 of  240 | Training Loss     0.061847 | Validation Loss     0.062321
Epoch  220 of  240 | Training Loss     0.062286 | Validation Loss     0.067333
Epoch  222 of  240 | Training Loss     0.061696 | Validation Loss     0.072534
Epoch  224 of  240 | Training Loss     0.062213 | Validation Loss     0.059975
Epoch  226 of  240 | Training Loss     0.061776 | Validation Loss     0.059754
Epoch  228 of  240 | Training Loss     0.061503 | Validation Loss     0.057097
Epoch  230 of  240 | Training Loss     0.061552 | Validation Loss     0.058293
Epoch  232 of  240 | Training Loss     0.060693 | Validation Loss     0.057776
Epoch  234 of  240 | Training Loss     0.061786 | Validation Loss     0.063850
Epoch  236 of  240 | Training Loss     0.061191 | Validation Loss     0.059186
Epoch  238 of  240 | Training Loss     0.061648 | Validation Loss     0.061238
COL_Y ['Vdc', 'Idc', 'Id', 'Iq']
Train time: 47338.43, Recent loss: 0.061440, RMS Errors: 0.0130 0.0124 0.0091 0.0020
                          MAE Errors: 0.0061 0.0038 0.0031 0.0011
(base) WE30866:hwpv mcde601$ 
