C:\src\pecblocks\examples\hwpv>python pv3_training.py ./ucf2ac/ucf2ac_config.json c:/data/ucf2.hdf5
model_folder = ./ucf2ac
model_root = ucf2a
data_path = c:/data/ucf2.hdf5
idx_in [0, 1, 2, 3, 4, 5, 6, 7, 8]
idx_out [9, 10, 11, 12]
read 1500 dataframes
dt=0.002000 data_len=2500 n_io=13 n_case=1500
['T', 'G', 'Fc', 'Md1', 'Mq1', 'Vod', 'Voq', 'GVrms', 'Ctl'] ['Vdc', 'Idc', 'Id', 'Iq'] (1500, 2500, 13)
shapes of t (2500,) data_train (1500, 2500, 13) n_in=9, n_out=4
t range 0.000000 to 4.998000
Before Scaling:
Column       Min       Max      Mean     Range
T         15.000    35.003    25.001    20.003
G         -0.000   999.995   483.449   999.995
Fc        55.000    65.000    60.003    10.000
Md1        0.800     1.200     1.002     0.400
Mq1       -0.499     0.501     0.001     1.000
Vod        0.000   345.797   171.304   345.797
Voq     -158.054   140.520    -4.992   298.574
GVrms     -0.000 410345.688 135669.844 410345.688
Ctl        0.000     1.000     0.799     1.000
Vdc        0.000   667.874   365.254   667.874
Idc        0.000     5.158     2.236     5.158
Id         0.000     6.188     3.068     6.188
Iq        -2.827     2.515    -0.089     5.343
After Scaling:
Column       Min       Max      Mean     Range     Scale    Offset
T         -0.500     0.500    -0.000     1.000    20.003    25.001
G         -0.483     0.517    -0.000     1.000   999.995   483.449
Fc        -0.500     0.500    -0.000     1.000    10.000    60.003
Md1       -0.505     0.495    -0.000     1.000     0.400     1.002
Mq1       -0.500     0.500    -0.000     1.000     1.000     0.001
Vod       -0.495     0.505     0.000     1.000   345.797   171.304
Voq       -0.513     0.487    -0.000     1.000   298.574    -4.992
GVrms     -0.331     0.669     0.000     1.000 410345.688 135669.844
Ctl       -0.799     0.201    -0.000     1.000     1.000     0.799
Vdc       -0.547     0.453    -0.000     1.000   667.874   365.254
Idc       -0.433     0.567     0.000     1.000     5.158     2.236
Id        -0.496     0.504     0.000     1.000     6.188     3.068
Iq        -0.512     0.488    -0.000     1.000     5.343    -0.089
make_mimo_block iir
Dataset split: 1500 1275 225 validation_scale=5.667
Epoch    0 of 2000 | Training Loss     1.773628 | Validation Loss     1.199572
Epoch   10 of 2000 | Training Loss     0.069958 | Validation Loss     0.061643
Epoch   20 of 2000 | Training Loss     0.019603 | Validation Loss     0.020812
Epoch   30 of 2000 | Training Loss     0.005553 | Validation Loss     0.005002
Epoch   40 of 2000 | Training Loss     0.002717 | Validation Loss     0.002618
Epoch   50 of 2000 | Training Loss     0.002032 | Validation Loss     0.001811
Epoch   60 of 2000 | Training Loss     0.001759 | Validation Loss     0.001539
Epoch   70 of 2000 | Training Loss     0.001517 | Validation Loss     0.001396
Epoch   80 of 2000 | Training Loss     0.001304 | Validation Loss     0.001167
Epoch   90 of 2000 | Training Loss     0.001106 | Validation Loss     0.001042
Epoch  100 of 2000 | Training Loss     0.000840 | Validation Loss     0.000808
Epoch  110 of 2000 | Training Loss     0.000687 | Validation Loss     0.000585
Epoch  120 of 2000 | Training Loss     0.000515 | Validation Loss     0.000521
Epoch  130 of 2000 | Training Loss     0.000467 | Validation Loss     0.000508
Epoch  140 of 2000 | Training Loss     0.000386 | Validation Loss     0.000399
Epoch  150 of 2000 | Training Loss     0.000399 | Validation Loss     0.000533
Epoch  160 of 2000 | Training Loss     0.000330 | Validation Loss     0.000390
Epoch  170 of 2000 | Training Loss     0.000353 | Validation Loss     0.000342
Epoch  180 of 2000 | Training Loss     0.000311 | Validation Loss     0.000436
Epoch  190 of 2000 | Training Loss     0.000334 | Validation Loss     0.000498
Epoch  200 of 2000 | Training Loss     0.000292 | Validation Loss     0.000286
Epoch  210 of 2000 | Training Loss     0.000319 | Validation Loss     0.000310
Epoch  220 of 2000 | Training Loss     0.000295 | Validation Loss     0.000342
Epoch  230 of 2000 | Training Loss     0.000385 | Validation Loss     0.000381
Epoch  240 of 2000 | Training Loss     0.000255 | Validation Loss     0.000292
Epoch  250 of 2000 | Training Loss     0.000305 | Validation Loss     0.000342
Epoch  260 of 2000 | Training Loss     0.000275 | Validation Loss     0.000278
Epoch  270 of 2000 | Training Loss     0.000246 | Validation Loss     0.000288
Epoch  280 of 2000 | Training Loss     0.000237 | Validation Loss     0.000214
Epoch  290 of 2000 | Training Loss     0.000217 | Validation Loss     0.000223
Epoch  300 of 2000 | Training Loss     0.000271 | Validation Loss     0.000246
Epoch  310 of 2000 | Training Loss     0.000199 | Validation Loss     0.000257
Epoch  320 of 2000 | Training Loss     0.000215 | Validation Loss     0.000202
Epoch  330 of 2000 | Training Loss     0.000336 | Validation Loss     0.000763
Epoch  340 of 2000 | Training Loss     0.000225 | Validation Loss     0.000243
Epoch  350 of 2000 | Training Loss     0.000290 | Validation Loss     0.000290
Epoch  360 of 2000 | Training Loss     0.000185 | Validation Loss     0.000197
Epoch  370 of 2000 | Training Loss     0.000310 | Validation Loss     0.000210
Epoch  380 of 2000 | Training Loss     0.000189 | Validation Loss     0.000195
Epoch  390 of 2000 | Training Loss     0.000251 | Validation Loss     0.000215
Epoch  400 of 2000 | Training Loss     0.000200 | Validation Loss     0.000178
Epoch  410 of 2000 | Training Loss     0.000258 | Validation Loss     0.000286
Epoch  420 of 2000 | Training Loss     0.000217 | Validation Loss     0.000272
Epoch  430 of 2000 | Training Loss     0.000205 | Validation Loss     0.000222
Epoch  440 of 2000 | Training Loss     0.000195 | Validation Loss     0.000279
Epoch  450 of 2000 | Training Loss     0.000162 | Validation Loss     0.000190
Epoch  460 of 2000 | Training Loss     0.000169 | Validation Loss     0.000207
Epoch  470 of 2000 | Training Loss     0.000173 | Validation Loss     0.000203
Epoch  480 of 2000 | Training Loss     0.000158 | Validation Loss     0.000162
Epoch  490 of 2000 | Training Loss     0.000203 | Validation Loss     0.000184
Epoch  500 of 2000 | Training Loss     0.000168 | Validation Loss     0.000160
Epoch  510 of 2000 | Training Loss     0.000161 | Validation Loss     0.000191
Epoch  520 of 2000 | Training Loss     0.000162 | Validation Loss     0.000153
Epoch  530 of 2000 | Training Loss     0.000200 | Validation Loss     0.000196
Epoch  540 of 2000 | Training Loss     0.000149 | Validation Loss     0.000174
Epoch  550 of 2000 | Training Loss     0.000165 | Validation Loss     0.000331
Epoch  560 of 2000 | Training Loss     0.000207 | Validation Loss     0.000154
Epoch  570 of 2000 | Training Loss     0.000156 | Validation Loss     0.000151
Epoch  580 of 2000 | Training Loss     0.000159 | Validation Loss     0.000212
Epoch  590 of 2000 | Training Loss     0.000141 | Validation Loss     0.000187
Epoch  600 of 2000 | Training Loss     0.000160 | Validation Loss     0.000153
Epoch  610 of 2000 | Training Loss     0.000144 | Validation Loss     0.000215
Epoch  620 of 2000 | Training Loss     0.000186 | Validation Loss     0.000170
Epoch  630 of 2000 | Training Loss     0.000199 | Validation Loss     0.000185
Epoch  640 of 2000 | Training Loss     0.000279 | Validation Loss     0.000179
Epoch  650 of 2000 | Training Loss     0.000150 | Validation Loss     0.000142
Epoch  660 of 2000 | Training Loss     0.000136 | Validation Loss     0.000163
Epoch  670 of 2000 | Training Loss     0.000185 | Validation Loss     0.000156
Epoch  680 of 2000 | Training Loss     0.000144 | Validation Loss     0.000157
Epoch  690 of 2000 | Training Loss     0.000130 | Validation Loss     0.000248
Epoch  700 of 2000 | Training Loss     0.000127 | Validation Loss     0.000155
Epoch  710 of 2000 | Training Loss     0.000197 | Validation Loss     0.000139
Epoch  720 of 2000 | Training Loss     0.000134 | Validation Loss     0.000165
Epoch  730 of 2000 | Training Loss     0.000125 | Validation Loss     0.000134
Epoch  740 of 2000 | Training Loss     0.000123 | Validation Loss     0.000123
Epoch  750 of 2000 | Training Loss     0.000139 | Validation Loss     0.000188
Epoch  760 of 2000 | Training Loss     0.000142 | Validation Loss     0.000173
Epoch  770 of 2000 | Training Loss     0.000122 | Validation Loss     0.000121
Epoch  780 of 2000 | Training Loss     0.000197 | Validation Loss     0.000188
Epoch  790 of 2000 | Training Loss     0.000135 | Validation Loss     0.000145
Epoch  800 of 2000 | Training Loss     0.000140 | Validation Loss     0.000153
Epoch  810 of 2000 | Training Loss     0.000142 | Validation Loss     0.000169
Epoch  820 of 2000 | Training Loss     0.000155 | Validation Loss     0.000166
Epoch  830 of 2000 | Training Loss     0.000123 | Validation Loss     0.000146
Epoch  840 of 2000 | Training Loss     0.000187 | Validation Loss     0.000168
Epoch  850 of 2000 | Training Loss     0.000143 | Validation Loss     0.000210
Epoch  860 of 2000 | Training Loss     0.000173 | Validation Loss     0.000139
Epoch  870 of 2000 | Training Loss     0.000155 | Validation Loss     0.000229
Epoch  880 of 2000 | Training Loss     0.000139 | Validation Loss     0.000126
Epoch  890 of 2000 | Training Loss     0.000135 | Validation Loss     0.000149
Epoch  900 of 2000 | Training Loss     0.000110 | Validation Loss     0.000135
Epoch  910 of 2000 | Training Loss     0.000126 | Validation Loss     0.000133
Epoch  920 of 2000 | Training Loss     0.000127 | Validation Loss     0.000162
Epoch  930 of 2000 | Training Loss     0.000113 | Validation Loss     0.000161
Epoch  940 of 2000 | Training Loss     0.000142 | Validation Loss     0.000160
Epoch  950 of 2000 | Training Loss     0.000133 | Validation Loss     0.000118
Epoch  960 of 2000 | Training Loss     0.000101 | Validation Loss     0.000140
Epoch  970 of 2000 | Training Loss     0.000123 | Validation Loss     0.000145
Epoch  980 of 2000 | Training Loss     0.000119 | Validation Loss     0.000133
Epoch  990 of 2000 | Training Loss     0.000112 | Validation Loss     0.000115
Epoch 1000 of 2000 | Training Loss     0.000117 | Validation Loss     0.000121
Epoch 1010 of 2000 | Training Loss     0.000125 | Validation Loss     0.000117
Epoch 1020 of 2000 | Training Loss     0.000103 | Validation Loss     0.000100
Epoch 1030 of 2000 | Training Loss     0.000141 | Validation Loss     0.000129
Epoch 1040 of 2000 | Training Loss     0.000119 | Validation Loss     0.000105
Epoch 1050 of 2000 | Training Loss     0.000113 | Validation Loss     0.000118
Epoch 1060 of 2000 | Training Loss     0.000131 | Validation Loss     0.000150
Epoch 1070 of 2000 | Training Loss     0.000097 | Validation Loss     0.000111
Epoch 1080 of 2000 | Training Loss     0.000170 | Validation Loss     0.000166
Epoch 1090 of 2000 | Training Loss     0.000099 | Validation Loss     0.000105
Epoch 1100 of 2000 | Training Loss     0.000115 | Validation Loss     0.000127
Epoch 1110 of 2000 | Training Loss     0.000167 | Validation Loss     0.000136
Epoch 1120 of 2000 | Training Loss     0.000102 | Validation Loss     0.000119
Epoch 1130 of 2000 | Training Loss     0.000153 | Validation Loss     0.000100
Epoch 1140 of 2000 | Training Loss     0.000117 | Validation Loss     0.000119
Epoch 1150 of 2000 | Training Loss     0.000108 | Validation Loss     0.000102
Epoch 1160 of 2000 | Training Loss     0.000108 | Validation Loss     0.000107
Epoch 1170 of 2000 | Training Loss     0.000156 | Validation Loss     0.000289
Epoch 1180 of 2000 | Training Loss     0.000168 | Validation Loss     0.000329
Epoch 1190 of 2000 | Training Loss     0.000125 | Validation Loss     0.000099
Epoch 1200 of 2000 | Training Loss     0.000107 | Validation Loss     0.000098
Epoch 1210 of 2000 | Training Loss     0.000118 | Validation Loss     0.000188
Epoch 1220 of 2000 | Training Loss     0.000118 | Validation Loss     0.000134
Epoch 1230 of 2000 | Training Loss     0.000147 | Validation Loss     0.000108
Epoch 1240 of 2000 | Training Loss     0.000177 | Validation Loss     0.000120
Epoch 1250 of 2000 | Training Loss     0.000128 | Validation Loss     0.000102
Epoch 1260 of 2000 | Training Loss     0.000197 | Validation Loss     0.000160
Epoch 1270 of 2000 | Training Loss     0.000123 | Validation Loss     0.000125
Epoch 1280 of 2000 | Training Loss     0.000120 | Validation Loss     0.000100
Epoch 1290 of 2000 | Training Loss     0.000097 | Validation Loss     0.000134
Epoch 1300 of 2000 | Training Loss     0.000138 | Validation Loss     0.000144
Epoch 1310 of 2000 | Training Loss     0.000093 | Validation Loss     0.000095
Epoch 1320 of 2000 | Training Loss     0.000115 | Validation Loss     0.000169
Epoch 1330 of 2000 | Training Loss     0.000129 | Validation Loss     0.000126
Epoch 1340 of 2000 | Training Loss     0.000115 | Validation Loss     0.000125
Epoch 1350 of 2000 | Training Loss     0.000132 | Validation Loss     0.000102
Epoch 1360 of 2000 | Training Loss     0.000104 | Validation Loss     0.000199
Epoch 1370 of 2000 | Training Loss     0.000136 | Validation Loss     0.000225
Epoch 1380 of 2000 | Training Loss     0.000094 | Validation Loss     0.000116
Epoch 1390 of 2000 | Training Loss     0.000115 | Validation Loss     0.000089
Epoch 1400 of 2000 | Training Loss     0.000079 | Validation Loss     0.000095
Epoch 1410 of 2000 | Training Loss     0.000094 | Validation Loss     0.000122
Epoch 1420 of 2000 | Training Loss     0.000106 | Validation Loss     0.000158
Epoch 1430 of 2000 | Training Loss     0.000091 | Validation Loss     0.000098
Epoch 1440 of 2000 | Training Loss     0.000141 | Validation Loss     0.000178
Epoch 1450 of 2000 | Training Loss     0.000099 | Validation Loss     0.000106
Epoch 1460 of 2000 | Training Loss     0.000091 | Validation Loss     0.000093
Epoch 1470 of 2000 | Training Loss     0.000078 | Validation Loss     0.000085
Epoch 1480 of 2000 | Training Loss     0.000090 | Validation Loss     0.000106
Epoch 1490 of 2000 | Training Loss     0.000091 | Validation Loss     0.000128
Epoch 1500 of 2000 | Training Loss     0.000104 | Validation Loss     0.000094
Epoch 1510 of 2000 | Training Loss     0.000095 | Validation Loss     0.000120
Epoch 1520 of 2000 | Training Loss     0.000075 | Validation Loss     0.000112
Epoch 1530 of 2000 | Training Loss     0.000101 | Validation Loss     0.000094
Epoch 1540 of 2000 | Training Loss     0.000092 | Validation Loss     0.000121
Epoch 1550 of 2000 | Training Loss     0.000109 | Validation Loss     0.000093
Epoch 1560 of 2000 | Training Loss     0.000168 | Validation Loss     0.000136
Epoch 1570 of 2000 | Training Loss     0.000094 | Validation Loss     0.000073
Epoch 1580 of 2000 | Training Loss     0.000139 | Validation Loss     0.000120
Epoch 1590 of 2000 | Training Loss     0.000107 | Validation Loss     0.000139
Epoch 1600 of 2000 | Training Loss     0.000091 | Validation Loss     0.000106
Epoch 1610 of 2000 | Training Loss     0.000080 | Validation Loss     0.000093
Epoch 1620 of 2000 | Training Loss     0.000093 | Validation Loss     0.000154
Epoch 1630 of 2000 | Training Loss     0.000089 | Validation Loss     0.000079
Epoch 1640 of 2000 | Training Loss     0.000088 | Validation Loss     0.000101
Epoch 1650 of 2000 | Training Loss     0.000090 | Validation Loss     0.000082
Epoch 1660 of 2000 | Training Loss     0.000088 | Validation Loss     0.000128
Epoch 1670 of 2000 | Training Loss     0.000117 | Validation Loss     0.000098
Epoch 1680 of 2000 | Training Loss     0.000144 | Validation Loss     0.000125
Epoch 1690 of 2000 | Training Loss     0.000095 | Validation Loss     0.000100
Epoch 1700 of 2000 | Training Loss     0.000076 | Validation Loss     0.000102
Epoch 1710 of 2000 | Training Loss     0.000097 | Validation Loss     0.000108
Epoch 1720 of 2000 | Training Loss     0.000086 | Validation Loss     0.000105
Epoch 1730 of 2000 | Training Loss     0.000097 | Validation Loss     0.000092
Epoch 1740 of 2000 | Training Loss     0.000088 | Validation Loss     0.000141
Epoch 1750 of 2000 | Training Loss     0.000111 | Validation Loss     0.000104
Epoch 1760 of 2000 | Training Loss     0.000093 | Validation Loss     0.000099
Epoch 1770 of 2000 | Training Loss     0.000077 | Validation Loss     0.000092
Epoch 1780 of 2000 | Training Loss     0.000081 | Validation Loss     0.000084
Epoch 1790 of 2000 | Training Loss     0.000118 | Validation Loss     0.000235
Epoch 1800 of 2000 | Training Loss     0.000098 | Validation Loss     0.000100
Epoch 1810 of 2000 | Training Loss     0.000086 | Validation Loss     0.000082
Epoch 1820 of 2000 | Training Loss     0.000096 | Validation Loss     0.000129
Epoch 1830 of 2000 | Training Loss     0.000110 | Validation Loss     0.000117
Epoch 1840 of 2000 | Training Loss     0.000133 | Validation Loss     0.000170
Epoch 1850 of 2000 | Training Loss     0.000088 | Validation Loss     0.000074
Epoch 1860 of 2000 | Training Loss     0.000105 | Validation Loss     0.000095
Epoch 1870 of 2000 | Training Loss     0.000076 | Validation Loss     0.000074
Epoch 1880 of 2000 | Training Loss     0.000092 | Validation Loss     0.000131
Epoch 1890 of 2000 | Training Loss     0.000094 | Validation Loss     0.000128
Epoch 1900 of 2000 | Training Loss     0.000068 | Validation Loss     0.000108
Epoch 1910 of 2000 | Training Loss     0.000072 | Validation Loss     0.000079
Epoch 1920 of 2000 | Training Loss     0.000075 | Validation Loss     0.000071
Epoch 1930 of 2000 | Training Loss     0.000100 | Validation Loss     0.000089
Epoch 1940 of 2000 | Training Loss     0.000069 | Validation Loss     0.000069
Epoch 1950 of 2000 | Training Loss     0.000100 | Validation Loss     0.000112
Epoch 1960 of 2000 | Training Loss     0.000071 | Validation Loss     0.000104
Epoch 1970 of 2000 | Training Loss     0.000233 | Validation Loss     0.000168
Epoch 1980 of 2000 | Training Loss     0.000092 | Validation Loss     0.000141
Epoch 1990 of 2000 | Training Loss     0.000082 | Validation Loss     0.000101
COL_Y ['Vdc', 'Idc', 'Id', 'Iq']
Train time: 23854.94, Recent loss: 0.000096, RMS Errors: 0.0086 0.0059 0.0071 0.0036
                          MAE Errors: 0.0014 0.0020 0.0010 0.0011
