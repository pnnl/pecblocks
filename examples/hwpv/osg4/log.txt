C:\src\pecblocks\examples\hwpv>python pv3_training.py ./osg4_vdvq/osg4_vdvq_config.json c:/data/osg4_vdvq.hdf5
model_folder = ./osg4_vdvq
model_root = osg4_vdvq
data_path = c:/data/osg4_vdvq.hdf5
idx_in [0, 1, 2, 3, 4, 5, 6, 7, 8]
idx_out [9, 10, 11, 12]
read 380 dataframes
dt=0.001000 data_len=7000 n_io=13 n_case=380
['T', 'G', 'Fc', 'Ud', 'Uq', 'Vd', 'Vq', 'GVrms', 'Ctl'] ['Vdc', 'Idc', 'Id', 'Iq'] (380, 7000, 13)
shapes of t (7000,) data_train (380, 7000, 13) n_in=9, n_out=4
t range 0.000000 to 6.999000
Before Scaling:
Column       Min       Max      Mean     Range
T         15.000    35.000    25.000    20.000
G         -0.000   999.995   465.430   999.995
Fc        55.000    65.000    60.000    10.000
Ud         0.800     1.100     0.995     0.300
Uq        -0.400     0.400     0.001     0.800
Vd        -0.000   441.253   249.850   441.253
Vq      -212.836    96.090   -47.965   308.925
GVrms     -0.000   268.020   102.081   268.020
Ctl        0.000     1.000     0.429     1.000
Vdc       -0.000   489.884   315.785   489.884
Idc       -0.000    32.890    14.824    32.890
Id        -0.280    83.285    32.691    83.565
Iq       -49.879    10.355    -7.265    60.234
After Scaling:
Column       Min       Max      Mean     Range     Scale    Offset
T         -0.500     0.500    -0.000     1.000    20.000    25.000
G         -0.465     0.535    -0.000     1.000   999.995   465.430
Fc        -0.500     0.500    -0.000     1.000    10.000    60.000
Ud        -0.652     0.348    -0.000     1.000     0.300     0.995
Uq        -0.501     0.499     0.000     1.000     0.800     0.001
Vd        -0.566     0.434     0.000     1.000   441.253   249.850
Vq        -0.534     0.466     0.000     1.000   308.925   -47.965
GVrms     -0.381     0.619     0.000     1.000   268.020   102.081
Ctl       -0.429     0.571     0.000     1.000     1.000     0.429
Vdc       -0.645     0.355     0.000     1.000   489.884   315.785
Idc       -0.451     0.549     0.000     1.000    32.890    14.824
Id        -0.395     0.605    -0.000     1.000    83.565    32.691
Iq        -0.707     0.293    -0.000     1.000    60.234    -7.265
make_mimo_block iir
Dataset split: 380 323 57 validation_scale=5.667
Epoch    0 of 2000 | Training Loss     1.460721 | Validation Loss     1.338628
Epoch   10 of 2000 | Training Loss     0.140561 | Validation Loss     0.131065
Epoch   20 of 2000 | Training Loss     0.032754 | Validation Loss     0.050471
Epoch   30 of 2000 | Training Loss     0.028438 | Validation Loss     0.044083
Epoch   40 of 2000 | Training Loss     0.016959 | Validation Loss     0.020165
Epoch   50 of 2000 | Training Loss     0.014918 | Validation Loss     0.017322
Epoch   60 of 2000 | Training Loss     0.012796 | Validation Loss     0.015103
Epoch   70 of 2000 | Training Loss     0.011098 | Validation Loss     0.012782
Epoch   80 of 2000 | Training Loss     0.010077 | Validation Loss     0.011445
Epoch   90 of 2000 | Training Loss     0.009076 | Validation Loss     0.010665
Epoch  100 of 2000 | Training Loss     0.008448 | Validation Loss     0.010111
Epoch  110 of 2000 | Training Loss     0.007788 | Validation Loss     0.009050
Epoch  120 of 2000 | Training Loss     0.007373 | Validation Loss     0.008244
Epoch  130 of 2000 | Training Loss     0.006814 | Validation Loss     0.007545
Epoch  140 of 2000 | Training Loss     0.006474 | Validation Loss     0.006838
Epoch  150 of 2000 | Training Loss     0.006003 | Validation Loss     0.006585
Epoch  160 of 2000 | Training Loss     0.005735 | Validation Loss     0.006067
Epoch  170 of 2000 | Training Loss     0.005571 | Validation Loss     0.005920
Epoch  180 of 2000 | Training Loss     0.005250 | Validation Loss     0.005541
Epoch  190 of 2000 | Training Loss     0.005255 | Validation Loss     0.005401
Epoch  200 of 2000 | Training Loss     0.004946 | Validation Loss     0.005397
Epoch  210 of 2000 | Training Loss     0.004712 | Validation Loss     0.005050
Epoch  220 of 2000 | Training Loss     0.004373 | Validation Loss     0.004817
Epoch  230 of 2000 | Training Loss     0.004377 | Validation Loss     0.004685
Epoch  240 of 2000 | Training Loss     0.003798 | Validation Loss     0.004422
Epoch  250 of 2000 | Training Loss     0.003671 | Validation Loss     0.004325
Epoch  260 of 2000 | Training Loss     0.003612 | Validation Loss     0.004172
Epoch  270 of 2000 | Training Loss     0.003458 | Validation Loss     0.003977
Epoch  280 of 2000 | Training Loss     0.003524 | Validation Loss     0.004019
Epoch  290 of 2000 | Training Loss     0.003375 | Validation Loss     0.003897
Epoch  300 of 2000 | Training Loss     0.003208 | Validation Loss     0.003740
Epoch  310 of 2000 | Training Loss     0.003161 | Validation Loss     0.003618
Epoch  320 of 2000 | Training Loss     0.003164 | Validation Loss     0.003795
Epoch  330 of 2000 | Training Loss     0.003106 | Validation Loss     0.003712
Epoch  340 of 2000 | Training Loss     0.002992 | Validation Loss     0.003567
Epoch  350 of 2000 | Training Loss     0.003088 | Validation Loss     0.003830
Epoch  360 of 2000 | Training Loss     0.003016 | Validation Loss     0.003699
Epoch  370 of 2000 | Training Loss     0.002907 | Validation Loss     0.003648
Epoch  380 of 2000 | Training Loss     0.002917 | Validation Loss     0.003541
Epoch  390 of 2000 | Training Loss     0.002925 | Validation Loss     0.003606
Epoch  400 of 2000 | Training Loss     0.002821 | Validation Loss     0.003691
Epoch  410 of 2000 | Training Loss     0.003076 | Validation Loss     0.003398
Epoch  420 of 2000 | Training Loss     0.002794 | Validation Loss     0.003506
Epoch  430 of 2000 | Training Loss     0.002734 | Validation Loss     0.003409
Epoch  440 of 2000 | Training Loss     0.002799 | Validation Loss     0.003378
Epoch  450 of 2000 | Training Loss     0.002666 | Validation Loss     0.003165
Epoch  460 of 2000 | Training Loss     0.002665 | Validation Loss     0.003183
Epoch  470 of 2000 | Training Loss     0.002872 | Validation Loss     0.003266
Epoch  480 of 2000 | Training Loss     0.002582 | Validation Loss     0.003107
Epoch  490 of 2000 | Training Loss     0.002786 | Validation Loss     0.003211
Epoch  500 of 2000 | Training Loss     0.002584 | Validation Loss     0.003118
Epoch  510 of 2000 | Training Loss     0.002628 | Validation Loss     0.003206
Epoch  520 of 2000 | Training Loss     0.002551 | Validation Loss     0.003146
Epoch  530 of 2000 | Training Loss     0.002559 | Validation Loss     0.003049
Epoch  540 of 2000 | Training Loss     0.002584 | Validation Loss     0.003193
Epoch  550 of 2000 | Training Loss     0.002568 | Validation Loss     0.003052
Epoch  560 of 2000 | Training Loss     0.002565 | Validation Loss     0.003433
Epoch  570 of 2000 | Training Loss     0.002817 | Validation Loss     0.002988
Epoch  580 of 2000 | Training Loss     0.002462 | Validation Loss     0.002931
Epoch  590 of 2000 | Training Loss     0.002430 | Validation Loss     0.003196
Epoch  600 of 2000 | Training Loss     0.002452 | Validation Loss     0.003024
Epoch  610 of 2000 | Training Loss     0.002506 | Validation Loss     0.003146
Epoch  620 of 2000 | Training Loss     0.002392 | Validation Loss     0.002982
Epoch  630 of 2000 | Training Loss     0.002459 | Validation Loss     0.002901
Epoch  640 of 2000 | Training Loss     0.002555 | Validation Loss     0.003306
Epoch  650 of 2000 | Training Loss     0.002416 | Validation Loss     0.002843
Epoch  660 of 2000 | Training Loss     0.002375 | Validation Loss     0.002997
Epoch  670 of 2000 | Training Loss     0.002408 | Validation Loss     0.002898
Epoch  680 of 2000 | Training Loss     0.002555 | Validation Loss     0.003627
Epoch  690 of 2000 | Training Loss     0.002389 | Validation Loss     0.002858
Epoch  700 of 2000 | Training Loss     0.002406 | Validation Loss     0.003079
Epoch  710 of 2000 | Training Loss     0.002342 | Validation Loss     0.002903
Epoch  720 of 2000 | Training Loss     0.002442 | Validation Loss     0.003027
Epoch  730 of 2000 | Training Loss     0.002334 | Validation Loss     0.003033
Epoch  740 of 2000 | Training Loss     0.002297 | Validation Loss     0.002840
Epoch  750 of 2000 | Training Loss     0.002294 | Validation Loss     0.002869
Epoch  760 of 2000 | Training Loss     0.002346 | Validation Loss     0.002882
Epoch  770 of 2000 | Training Loss     0.002234 | Validation Loss     0.002772
Epoch  780 of 2000 | Training Loss     0.002275 | Validation Loss     0.002784
Epoch  790 of 2000 | Training Loss     0.002285 | Validation Loss     0.002871
Epoch  800 of 2000 | Training Loss     0.002359 | Validation Loss     0.002737
Epoch  810 of 2000 | Training Loss     0.002208 | Validation Loss     0.002715
Epoch  820 of 2000 | Training Loss     0.002228 | Validation Loss     0.002660
Epoch  830 of 2000 | Training Loss     0.002203 | Validation Loss     0.002711
Epoch  840 of 2000 | Training Loss     0.002316 | Validation Loss     0.002801
Epoch  850 of 2000 | Training Loss     0.002159 | Validation Loss     0.002654
Epoch  860 of 2000 | Training Loss     0.002246 | Validation Loss     0.002728
Epoch  870 of 2000 | Training Loss     0.002164 | Validation Loss     0.002720
Epoch  880 of 2000 | Training Loss     0.002146 | Validation Loss     0.002710
Epoch  890 of 2000 | Training Loss     0.002493 | Validation Loss     0.003440
Epoch  900 of 2000 | Training Loss     0.002145 | Validation Loss     0.002714
Epoch  910 of 2000 | Training Loss     0.002227 | Validation Loss     0.002705
Epoch  920 of 2000 | Training Loss     0.002129 | Validation Loss     0.002723
Epoch  930 of 2000 | Training Loss     0.002139 | Validation Loss     0.002585
Epoch  940 of 2000 | Training Loss     0.002184 | Validation Loss     0.002661
Epoch  950 of 2000 | Training Loss     0.002137 | Validation Loss     0.002545
Epoch  960 of 2000 | Training Loss     0.002111 | Validation Loss     0.002623
Epoch  970 of 2000 | Training Loss     0.002161 | Validation Loss     0.002621
Epoch  980 of 2000 | Training Loss     0.002067 | Validation Loss     0.002552
Epoch  990 of 2000 | Training Loss     0.002065 | Validation Loss     0.002567
Epoch 1000 of 2000 | Training Loss     0.002072 | Validation Loss     0.002498
Epoch 1010 of 2000 | Training Loss     0.002109 | Validation Loss     0.002575
Epoch 1020 of 2000 | Training Loss     0.002036 | Validation Loss     0.002538
Epoch 1030 of 2000 | Training Loss     0.002224 | Validation Loss     0.003093
Epoch 1040 of 2000 | Training Loss     0.002065 | Validation Loss     0.002522
Epoch 1050 of 2000 | Training Loss     0.002042 | Validation Loss     0.002498
Epoch 1060 of 2000 | Training Loss     0.002210 | Validation Loss     0.002524
Epoch 1070 of 2000 | Training Loss     0.002046 | Validation Loss     0.002459
Epoch 1080 of 2000 | Training Loss     0.002094 | Validation Loss     0.002460
Epoch 1090 of 2000 | Training Loss     0.002270 | Validation Loss     0.002574
Epoch 1100 of 2000 | Training Loss     0.002066 | Validation Loss     0.002627
Epoch 1110 of 2000 | Training Loss     0.002047 | Validation Loss     0.002491
Epoch 1120 of 2000 | Training Loss     0.002004 | Validation Loss     0.002570
Epoch 1130 of 2000 | Training Loss     0.001999 | Validation Loss     0.002571
Epoch 1140 of 2000 | Training Loss     0.002009 | Validation Loss     0.002448
Epoch 1150 of 2000 | Training Loss     0.002010 | Validation Loss     0.002507
Epoch 1160 of 2000 | Training Loss     0.002026 | Validation Loss     0.002476
Epoch 1170 of 2000 | Training Loss     0.002006 | Validation Loss     0.002455
Epoch 1180 of 2000 | Training Loss     0.002008 | Validation Loss     0.002519
Epoch 1190 of 2000 | Training Loss     0.002230 | Validation Loss     0.002488
Epoch 1200 of 2000 | Training Loss     0.001970 | Validation Loss     0.002368
Epoch 1210 of 2000 | Training Loss     0.002022 | Validation Loss     0.002480
Epoch 1220 of 2000 | Training Loss     0.002054 | Validation Loss     0.002598
Epoch 1230 of 2000 | Training Loss     0.001988 | Validation Loss     0.002438
Epoch 1240 of 2000 | Training Loss     0.002020 | Validation Loss     0.002463
Epoch 1250 of 2000 | Training Loss     0.001968 | Validation Loss     0.002415
Epoch 1260 of 2000 | Training Loss     0.002073 | Validation Loss     0.002370
Epoch 1270 of 2000 | Training Loss     0.001960 | Validation Loss     0.002413
Epoch 1280 of 2000 | Training Loss     0.001941 | Validation Loss     0.002339
Epoch 1290 of 2000 | Training Loss     0.001919 | Validation Loss     0.002372
Epoch 1300 of 2000 | Training Loss     0.002045 | Validation Loss     0.002612
Epoch 1310 of 2000 | Training Loss     0.001987 | Validation Loss     0.002359
Epoch 1320 of 2000 | Training Loss     0.001966 | Validation Loss     0.002483
Epoch 1330 of 2000 | Training Loss     0.002025 | Validation Loss     0.002443
Epoch 1340 of 2000 | Training Loss     0.001942 | Validation Loss     0.002404
Epoch 1350 of 2000 | Training Loss     0.001948 | Validation Loss     0.002418
Epoch 1360 of 2000 | Training Loss     0.002030 | Validation Loss     0.002363
Epoch 1370 of 2000 | Training Loss     0.002076 | Validation Loss     0.002325
Epoch 1380 of 2000 | Training Loss     0.001962 | Validation Loss     0.002325
Epoch 1390 of 2000 | Training Loss     0.001973 | Validation Loss     0.002384
Epoch 1400 of 2000 | Training Loss     0.001956 | Validation Loss     0.002340
Epoch 1410 of 2000 | Training Loss     0.001954 | Validation Loss     0.002448
Epoch 1420 of 2000 | Training Loss     0.001931 | Validation Loss     0.002408
Epoch 1430 of 2000 | Training Loss     0.001953 | Validation Loss     0.002283
Epoch 1440 of 2000 | Training Loss     0.002097 | Validation Loss     0.002672
Epoch 1450 of 2000 | Training Loss     0.002015 | Validation Loss     0.002360
Epoch 1460 of 2000 | Training Loss     0.001891 | Validation Loss     0.002304
Epoch 1470 of 2000 | Training Loss     0.001897 | Validation Loss     0.002279
Epoch 1480 of 2000 | Training Loss     0.001888 | Validation Loss     0.002285
Epoch 1490 of 2000 | Training Loss     0.001870 | Validation Loss     0.002302
Epoch 1500 of 2000 | Training Loss     0.002014 | Validation Loss     0.002399
Epoch 1510 of 2000 | Training Loss     0.001909 | Validation Loss     0.002387
Epoch 1520 of 2000 | Training Loss     0.001879 | Validation Loss     0.002317
Epoch 1530 of 2000 | Training Loss     0.001854 | Validation Loss     0.002259
Epoch 1540 of 2000 | Training Loss     0.002132 | Validation Loss     0.002992
Epoch 1550 of 2000 | Training Loss     0.001869 | Validation Loss     0.002291
Epoch 1560 of 2000 | Training Loss     0.001867 | Validation Loss     0.002256
Epoch 1570 of 2000 | Training Loss     0.001861 | Validation Loss     0.002222
Epoch 1580 of 2000 | Training Loss     0.001883 | Validation Loss     0.002416
Epoch 1590 of 2000 | Training Loss     0.001863 | Validation Loss     0.002319
Epoch 1600 of 2000 | Training Loss     0.001862 | Validation Loss     0.002258
Epoch 1610 of 2000 | Training Loss     0.001822 | Validation Loss     0.002237
Epoch 1620 of 2000 | Training Loss     0.001905 | Validation Loss     0.002267
Epoch 1630 of 2000 | Training Loss     0.001875 | Validation Loss     0.002248
Epoch 1640 of 2000 | Training Loss     0.001966 | Validation Loss     0.002329
Epoch 1650 of 2000 | Training Loss     0.001938 | Validation Loss     0.002228
Epoch 1660 of 2000 | Training Loss     0.001860 | Validation Loss     0.002399
Epoch 1670 of 2000 | Training Loss     0.001846 | Validation Loss     0.002250
Epoch 1680 of 2000 | Training Loss     0.001847 | Validation Loss     0.002206
Epoch 1690 of 2000 | Training Loss     0.001874 | Validation Loss     0.002233
Epoch 1700 of 2000 | Training Loss     0.001801 | Validation Loss     0.002170
Epoch 1710 of 2000 | Training Loss     0.001918 | Validation Loss     0.002300
Epoch 1720 of 2000 | Training Loss     0.001817 | Validation Loss     0.002154
Epoch 1730 of 2000 | Training Loss     0.001835 | Validation Loss     0.002181
Epoch 1740 of 2000 | Training Loss     0.001804 | Validation Loss     0.002203
Epoch 1750 of 2000 | Training Loss     0.001875 | Validation Loss     0.002211
Epoch 1760 of 2000 | Training Loss     0.001837 | Validation Loss     0.002153
Epoch 1770 of 2000 | Training Loss     0.001873 | Validation Loss     0.002210
Epoch 1780 of 2000 | Training Loss     0.001833 | Validation Loss     0.002177
Epoch 1790 of 2000 | Training Loss     0.001774 | Validation Loss     0.002158
Epoch 1800 of 2000 | Training Loss     0.001820 | Validation Loss     0.002121
Epoch 1810 of 2000 | Training Loss     0.001878 | Validation Loss     0.002170
Epoch 1820 of 2000 | Training Loss     0.001783 | Validation Loss     0.002171
Epoch 1830 of 2000 | Training Loss     0.001794 | Validation Loss     0.002192
Epoch 1840 of 2000 | Training Loss     0.001793 | Validation Loss     0.002140
Epoch 1850 of 2000 | Training Loss     0.001802 | Validation Loss     0.002115
Epoch 1860 of 2000 | Training Loss     0.001810 | Validation Loss     0.002222
Epoch 1870 of 2000 | Training Loss     0.001775 | Validation Loss     0.002162
Epoch 1880 of 2000 | Training Loss     0.001805 | Validation Loss     0.002161
Epoch 1890 of 2000 | Training Loss     0.001997 | Validation Loss     0.002278
Epoch 1900 of 2000 | Training Loss     0.001759 | Validation Loss     0.002106
Epoch 1910 of 2000 | Training Loss     0.001787 | Validation Loss     0.002132
Epoch 1920 of 2000 | Training Loss     0.002188 | Validation Loss     0.002289
Epoch 1930 of 2000 | Training Loss     0.001759 | Validation Loss     0.002100
Epoch 1940 of 2000 | Training Loss     0.001762 | Validation Loss     0.002130
Epoch 1950 of 2000 | Training Loss     0.001749 | Validation Loss     0.002196
Epoch 1960 of 2000 | Training Loss     0.001745 | Validation Loss     0.002162
Epoch 1970 of 2000 | Training Loss     0.001785 | Validation Loss     0.002171
Epoch 1980 of 2000 | Training Loss     0.001784 | Validation Loss     0.002168
Epoch 1990 of 2000 | Training Loss     0.001757 | Validation Loss     0.002076
COL_Y ['Vdc', 'Idc', 'Id', 'Iq']
Train time: 24138.62, Recent loss: 0.001779, RMS Errors: 0.0196 0.0130 0.0103 0.0043
                          MAE Errors: 0.0111 0.0048 0.0035 0.0017
