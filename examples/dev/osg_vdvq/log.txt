C:\src\pecblocks\examples\hwpv>python pv3_training.py
model_folder = ./osg_vdvq
model_root = osg_vdvq
data_path = ./data/osg_vdvq.hdf5
idx_in [0, 1, 2, 3, 4, 5, 6, 7, 8]
idx_out [9, 10, 11, 12]
read 80 dataframes
dt=0.001000 data_len=5000 n_io=13 n_case=80
['T', 'G', 'Fc', 'Ud', 'Uq', 'Vd', 'Vq', 'GVrms', 'Ctl'] ['Vdc', 'Idc', 'Id', 'Iq'] (80, 5000, 13)
shapes of t (5000,) data_train (80, 5000, 13) n_in=9, n_out=4
t range 0.000000 to 4.999000
Before Scaling:
Column       Min       Max      Mean     Range
T         15.000    35.000    25.000    20.000
G         -0.000   999.995   552.635   999.995
Fc        55.000    65.000    60.000    10.000
Ud         0.900     1.100     1.000     0.200
Uq        -0.400     0.400     0.001     0.800
Vd        -0.000   376.486   266.585   376.486
Vq      -196.241   118.435   -34.337   314.676
GVrms     -0.000   267.385   120.627   267.385
Ctl        0.000     1.000     0.499     1.000
Vdc       -0.000   455.263   333.523   455.263
Idc       -0.000    32.890    17.632    32.890
Id        -0.280    86.004    39.211    86.284
Iq       -45.990    15.855    -6.054    61.845
After Scaling:
Column       Min       Max      Mean     Range     Scale    Offset
T         -0.500     0.500    -0.000     1.000    20.000    25.000
G         -0.553     0.447    -0.000     1.000   999.995   552.635
Fc        -0.500     0.500    -0.000     1.000    10.000    60.000
Ud        -0.500     0.500    -0.000     1.000     0.200     1.000
Uq        -0.501     0.499    -0.000     1.000     0.800     0.001
Vd        -0.708     0.292    -0.000     1.000   376.486   266.585
Vq        -0.515     0.485     0.000     1.000   314.676   -34.337
GVrms     -0.451     0.549     0.000     1.000   267.385   120.627
Ctl       -0.499     0.501     0.000     1.000     1.000     0.499
Vdc       -0.733     0.267     0.000     1.000   455.263   333.523
Idc       -0.536     0.464     0.000     1.000    32.890    17.632
Id        -0.458     0.542     0.000     1.000    86.284    39.211
Iq        -0.646     0.354     0.000     1.000    61.845    -6.054
make_mimo_block iir
Iter    0 of 2000 | Loss     0.100286
Iter   10 of 2000 | Loss     0.078389
Iter   20 of 2000 | Loss     0.066206
Iter   30 of 2000 | Loss     0.060797
Iter   40 of 2000 | Loss     0.051426
Iter   50 of 2000 | Loss     0.034106
Iter   60 of 2000 | Loss     0.020233
Iter   70 of 2000 | Loss     0.017177
Iter   80 of 2000 | Loss     0.015788
Iter   90 of 2000 | Loss     0.015095
Iter  100 of 2000 | Loss     0.014198
Iter  110 of 2000 | Loss     0.013168
Iter  120 of 2000 | Loss     0.011718
Iter  130 of 2000 | Loss     0.009672
Iter  140 of 2000 | Loss     0.006992
Iter  150 of 2000 | Loss     0.004720
Iter  160 of 2000 | Loss     0.003781
Iter  170 of 2000 | Loss     0.003542
Iter  180 of 2000 | Loss     0.003492
Iter  190 of 2000 | Loss     0.003452
Iter  200 of 2000 | Loss     0.003424
Iter  210 of 2000 | Loss     0.003403
Iter  220 of 2000 | Loss     0.003385
Iter  230 of 2000 | Loss     0.003367
Iter  240 of 2000 | Loss     0.003350
Iter  250 of 2000 | Loss     0.003333
Iter  260 of 2000 | Loss     0.003316
Iter  270 of 2000 | Loss     0.003299
Iter  280 of 2000 | Loss     0.003281
Iter  290 of 2000 | Loss     0.003262
Iter  300 of 2000 | Loss     0.003243
Iter  310 of 2000 | Loss     0.003222
Iter  320 of 2000 | Loss     0.003198
Iter  330 of 2000 | Loss     0.003171
Iter  340 of 2000 | Loss     0.003138
Iter  350 of 2000 | Loss     0.003097
Iter  360 of 2000 | Loss     0.003040
Iter  370 of 2000 | Loss     0.002959
Iter  380 of 2000 | Loss     0.002833
Iter  390 of 2000 | Loss     0.002628
Iter  400 of 2000 | Loss     0.002299
Iter  410 of 2000 | Loss     0.001847
Iter  420 of 2000 | Loss     0.001447
Iter  430 of 2000 | Loss     0.001310
Iter  440 of 2000 | Loss     0.001275
Iter  450 of 2000 | Loss     0.001235
Iter  460 of 2000 | Loss     0.001210
Iter  470 of 2000 | Loss     0.001191
Iter  480 of 2000 | Loss     0.001172
Iter  490 of 2000 | Loss     0.001153
Iter  500 of 2000 | Loss     0.001135
Iter  510 of 2000 | Loss     0.001118
Iter  520 of 2000 | Loss     0.001100
Iter  530 of 2000 | Loss     0.001083
Iter  540 of 2000 | Loss     0.001066
Iter  550 of 2000 | Loss     0.001049
Iter  560 of 2000 | Loss     0.001032
Iter  570 of 2000 | Loss     0.001015
Iter  580 of 2000 | Loss     0.000998
Iter  590 of 2000 | Loss     0.000981
Iter  600 of 2000 | Loss     0.000965
Iter  610 of 2000 | Loss     0.000948
Iter  620 of 2000 | Loss     0.000932
Iter  630 of 2000 | Loss     0.000915
Iter  640 of 2000 | Loss     0.000899
Iter  650 of 2000 | Loss     0.000883
Iter  660 of 2000 | Loss     0.000866
Iter  670 of 2000 | Loss     0.000850
Iter  680 of 2000 | Loss     0.000834
Iter  690 of 2000 | Loss     0.000819
Iter  700 of 2000 | Loss     0.000803
Iter  710 of 2000 | Loss     0.000787
Iter  720 of 2000 | Loss     0.000772
Iter  730 of 2000 | Loss     0.000756
Iter  740 of 2000 | Loss     0.000741
Iter  750 of 2000 | Loss     0.000726
Iter  760 of 2000 | Loss     0.000711
Iter  770 of 2000 | Loss     0.000696
Iter  780 of 2000 | Loss     0.000682
Iter  790 of 2000 | Loss     0.000668
Iter  800 of 2000 | Loss     0.000654
Iter  810 of 2000 | Loss     0.000641
Iter  820 of 2000 | Loss     0.000627
Iter  830 of 2000 | Loss     0.000614
Iter  840 of 2000 | Loss     0.000602
Iter  850 of 2000 | Loss     0.000590
Iter  860 of 2000 | Loss     0.000579
Iter  870 of 2000 | Loss     0.000568
Iter  880 of 2000 | Loss     0.000557
Iter  890 of 2000 | Loss     0.000548
Iter  900 of 2000 | Loss     0.000537
Iter  910 of 2000 | Loss     0.000527
Iter  920 of 2000 | Loss     0.000519
Iter  930 of 2000 | Loss     0.000510
Iter  940 of 2000 | Loss     0.000501
Iter  950 of 2000 | Loss     0.000493
Iter  960 of 2000 | Loss     0.000484
Iter  970 of 2000 | Loss     0.000477
Iter  980 of 2000 | Loss     0.000470
Iter  990 of 2000 | Loss     0.000463
Iter 1000 of 2000 | Loss     0.000455
Iter 1010 of 2000 | Loss     0.000448
Iter 1020 of 2000 | Loss     0.000442
Iter 1030 of 2000 | Loss     0.000436
Iter 1040 of 2000 | Loss     0.000430
Iter 1050 of 2000 | Loss     0.000425
Iter 1060 of 2000 | Loss     0.000418
Iter 1070 of 2000 | Loss     0.000413
Iter 1080 of 2000 | Loss     0.000408
Iter 1090 of 2000 | Loss     0.000403
Iter 1100 of 2000 | Loss     0.000402
Iter 1110 of 2000 | Loss     0.000394
Iter 1120 of 2000 | Loss     0.000390
Iter 1130 of 2000 | Loss     0.000384
Iter 1140 of 2000 | Loss     0.000380
Iter 1150 of 2000 | Loss     0.000376
Iter 1160 of 2000 | Loss     0.000375
Iter 1170 of 2000 | Loss     0.000370
Iter 1180 of 2000 | Loss     0.000365
Iter 1190 of 2000 | Loss     0.000361
Iter 1200 of 2000 | Loss     0.000358
Iter 1210 of 2000 | Loss     0.000356
Iter 1220 of 2000 | Loss     0.000351
Iter 1230 of 2000 | Loss     0.000348
Iter 1240 of 2000 | Loss     0.000345
Iter 1250 of 2000 | Loss     0.000345
Iter 1260 of 2000 | Loss     0.000339
Iter 1270 of 2000 | Loss     0.000336
Iter 1280 of 2000 | Loss     0.000333
Iter 1290 of 2000 | Loss     0.000331
Iter 1300 of 2000 | Loss     0.000328
Iter 1310 of 2000 | Loss     0.000332
Iter 1320 of 2000 | Loss     0.000326
Iter 1330 of 2000 | Loss     0.000321
Iter 1340 of 2000 | Loss     0.000319
Iter 1350 of 2000 | Loss     0.000317
Iter 1360 of 2000 | Loss     0.000315
Iter 1370 of 2000 | Loss     0.000313
Iter 1380 of 2000 | Loss     0.000311
Iter 1390 of 2000 | Loss     0.000309
Iter 1400 of 2000 | Loss     0.000312
Iter 1410 of 2000 | Loss     0.000305
Iter 1420 of 2000 | Loss     0.000304
Iter 1430 of 2000 | Loss     0.000302
Iter 1440 of 2000 | Loss     0.000300
Iter 1450 of 2000 | Loss     0.000298
Iter 1460 of 2000 | Loss     0.000297
Iter 1470 of 2000 | Loss     0.000295
Iter 1480 of 2000 | Loss     0.000294
Iter 1490 of 2000 | Loss     0.000292
Iter 1500 of 2000 | Loss     0.000291
Iter 1510 of 2000 | Loss     0.000296
Iter 1520 of 2000 | Loss     0.000291
Iter 1530 of 2000 | Loss     0.000288
Iter 1540 of 2000 | Loss     0.000285
Iter 1550 of 2000 | Loss     0.000284
Iter 1560 of 2000 | Loss     0.000283
Iter 1570 of 2000 | Loss     0.000281
Iter 1580 of 2000 | Loss     0.000280
Iter 1590 of 2000 | Loss     0.000279
Iter 1600 of 2000 | Loss     0.000278
Iter 1610 of 2000 | Loss     0.000276
Iter 1620 of 2000 | Loss     0.000275
Iter 1630 of 2000 | Loss     0.000274
Iter 1640 of 2000 | Loss     0.000273
Iter 1650 of 2000 | Loss     0.000272
Iter 1660 of 2000 | Loss     0.000271
Iter 1670 of 2000 | Loss     0.000270
Iter 1680 of 2000 | Loss     0.000269
Iter 1690 of 2000 | Loss     0.000276
Iter 1700 of 2000 | Loss     0.000270
Iter 1710 of 2000 | Loss     0.000266
Iter 1720 of 2000 | Loss     0.000264
Iter 1730 of 2000 | Loss     0.000263
Iter 1740 of 2000 | Loss     0.000262
Iter 1750 of 2000 | Loss     0.000261
Iter 1760 of 2000 | Loss     0.000260
Iter 1770 of 2000 | Loss     0.000259
Iter 1780 of 2000 | Loss     0.000258
Iter 1790 of 2000 | Loss     0.000257
Iter 1800 of 2000 | Loss     0.000256
Iter 1810 of 2000 | Loss     0.000255
Iter 1820 of 2000 | Loss     0.000254
Iter 1830 of 2000 | Loss     0.000253
Iter 1840 of 2000 | Loss     0.000252
Iter 1850 of 2000 | Loss     0.000251
Iter 1860 of 2000 | Loss     0.000250
Iter 1870 of 2000 | Loss     0.000249
Iter 1880 of 2000 | Loss     0.000248
Iter 1890 of 2000 | Loss     0.000246
Iter 1900 of 2000 | Loss     0.000246
Iter 1910 of 2000 | Loss     0.000244
Iter 1920 of 2000 | Loss     0.000243
Iter 1930 of 2000 | Loss     0.000242
Iter 1940 of 2000 | Loss     0.000241
Iter 1950 of 2000 | Loss     0.000240
Iter 1960 of 2000 | Loss     0.000238
Iter 1970 of 2000 | Loss     0.000237
Iter 1980 of 2000 | Loss     0.000236
Iter 1990 of 2000 | Loss     0.000235
COL_Y ['Vdc', 'Idc', 'Id', 'Iq']
Train time: 5052.91, Recent loss: 0.000234, RMS Errors: 0.0304 0.0173 0.0188 0.0087
                          MAE Errors: 0.0166 0.0094 0.0120 0.0046

